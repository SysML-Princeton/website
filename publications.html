<!DOCTYPE html>

<html lang="en-us">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <!-- Font Awesome for social media icons -->
    <script src="https://kit.fontawesome.com/791291c78f.js" crossorigin="anonymous"></script>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <!-- Site Information -->
    <title> SAIL@Princeton </title>

    <style type="text/css">
      .smlinks {
        color: black;
      }
      .smlinks:hover {
        color: rgb(7, 107, 255);
      }
      .paper-item {
        margin-bottom: 15px; /* Adjust this value to increase/decrease the space */
      }
    </style>

    <!-- Favicon -->
    <!-- TODO(ruipan): we could add a favicon of the website here -->
    <!-- https://realfavicongenerator.net/ -->
    <!-- <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff"> -->
    
    <!-- Functionality for searching papers -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
      $(document).ready(function () {
          // Function to get URL parameters
          function getQueryParam(name) {
              let urlParams = new URLSearchParams(window.location.search);
              return urlParams.get(name) || "";
          }
    
          // Function to perform search
          function filterPapers(query) {
              query = query.toLowerCase();
              $(".paper-item").each(function () {
                  let text = $(this).text().toLowerCase();
                  $(this).closest("li").toggle(text.includes(query));
              });
          }
    
          // Populate search bar and apply filter if "search" parameter exists
          let searchQuery = getQueryParam("search");
          if (searchQuery) {
              $("#search").val(searchQuery);
              filterPapers(searchQuery); // Directly apply the filter
          }
    
          // Attach event listener for manual searches
          $("#search").on("input", function () {
              filterPapers($(this).val());
          });
      });
    </script>
  </head>

  <body>
    <!-- Nav Bar -->
    <!-- TODO(ruipan): figure out how to align the nav items to the right rather than the left -->
    <nav class="navbar navbar-expand-lg navbar-light sticky-top navbar-custom" style="background-color: #f58025">
        <a class="navbar-brand" href="index.html">
          <img src="./images/princeton_square.jpeg" width="30" height="30" class="d-inline-block align-top">
          SAIL@Princeton
        </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="index.html#projects">Projects</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="people.html">People</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="publications.html">Publications</a>
          </li>
        </ul>
      </div>
    </nav>

    <!-- Jumbotron -->
    <div class="jumbotron jumbotron-fluid text-center">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-sm-12">
            <h2 class="jumbotron-heading">Publications of SAIL@Princeton</h2>
            <p class="lead">TODO: write a sentence or two to describe this group of people. Something like: SAIL@Princeton publishes across conferences, from systemsy ones to ML ones</p>
          </div>
        </div>
      </div>
    </div>

    <!-- Search bar -->
    <div class="container">
        <div class="row">
          <div class="col-sm-12">
            <input type="text" id="search" class="form-control mb-3" placeholder="Search by title, author, or keyword...">
          </div>
        </div>
    </div>

    <!-- 2025 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2025</h3>
          <ul>
            <li class="paper-item">
              <h5>Marconi: Prefix Caching for the Era of Hybrid LLMs</h5> 
              Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali <br>
              <a href="https://mlsys.org/Conferences/2025" target="_blank">MLSys 2025</a><br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <span class="badge badge-secondary">State Space Models</span>
              <div class="mt-2">
                <a href="publications/arxiv24_marconi.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#marconi-abstract" role="button" aria-expanded="false" aria-controls="marconi-abstract">Abstract</a>
                <a href="javascript:void(0);" target="_blank">
                  <button type="button" class="btn btn-outline-secondary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="marconi-abstract">
                <div class="card card-body">
                  Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent
                  layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language
                  Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency
                  optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of
                  in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and
                  instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most
                  of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix
                  caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously
                  assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a
                  taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints.
                  Across diverse workloads and Hybrid models, Marconi achieves up to 34.4× higher token hit rates (71.1% or 617
                  ms lower TTFT) compared to state-of-the-art prefix caching systems.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Mowgli: A Passive Approach to Learning Rate Control for Real-Time Video</h5> 
              Neil Agarwal, Rui Pan, Francis Y. Yan, Ravi Netravali <br>
              <a href="https://www.usenix.org/conference/nsdi25" target="_blank">USENIX NSDI 2025</a><br>
              <span class="badge badge-secondary">ML for Systems</span>
              <div class="mt-2">
                <a href="publications/arxiv24_mowgli.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#mowgli-abstract" role="button" aria-expanded="false" aria-controls="mowgli-abstract">Abstract</a>
              </div>
              <div class="collapse" id="mowgli-abstract">
                <div class="card card-body">
                  Rate control algorithms are at the heart of video conferencing platforms, 
                  determining target bitrates that match dynamic network characteristics for high quality. 
                  Recent data-driven strategies have shown promise for this challenging task, 
                  but the performance degradation they introduce during training has been a nonstarter 
                  for many production services, precluding adoption. 
                  This paper aims to bolster the practicality of data-driven rate control by presenting 
                  an alternative avenue for experiential learning: 
                  leveraging purely existing telemetry logs produced by the incumbent algorithm in production. 
                  We observe that these logs often contain effective decisions, although often at the wrong times or in the wrong order. 
                  To realize this approach despite the inherent uncertainty that log-based learning brings 
                  (i.e., lack of feedback for new decisions), our system, Mowgli, 
                  combines a variety of robust learning techniques (i.e., conservatively reasoning 
                  about alternate behavior to minimize risk and using a richer model formulation to account for environmental noise). 
                  Across diverse networks (emulated and real-world), Mowgli outperforms the widely deployed GCC algorithm, 
                  increasing average video bitrates by 15-39% while reducing freeze rates by 60-100%.
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- 2024 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2024</h3>
          <ul>
            <li class="paper-item">
              <h5>
                Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving
                <img src="images/acm_available_1.1.png" height="25"/><img src="images/acm_functional_1.1.png" height="25"/><img src="images/acm_reproduced_1.1.png" height="25"/>
              </h5> 
              Yinwei Dai*, Rui Pan*, Anand Iyer, Kai Li, Ravi Netravali <br>
              <a href="https://sigops.org/s/conferences/sosp/2024/" target="_blank">ACM SOSP 2024</a><br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <div class="mt-2">
                <a href="publications/sosp24_apparate.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#apparate-abstract" role="button" aria-expanded="false" aria-controls="apparate-abstract">Abstract</a>
                <a href="https://github.com/dywsjtu/apparate" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="apparate-abstract">
                <div class="card card-body">
                  Machine learning (ML) inference platforms are tasked with balancing two competing goals: 
                  ensuring high throughput given many requests, and delivering low-latency responses to support interactive applications. 
                  Unfortunately, existing platform knobs (e.g., batch sizes) fail to ease this fundamental tension, 
                  and instead only enable users to harshly trade off one property for the other. 
                  This paper explores an alternate strategy to taming throughput-latency tradeoffs by changing the granularity 
                  at which inference is performed. 
                  We present Apparate, a system that automatically applies and manages early exits (EEs) in ML models, 
                  whereby certain inputs can exit with results at intermediate layers. 
                  To cope with the time-varying overhead and accuracy challenges that EEs bring, 
                  Apparate repurposes exits to provide continual feedback that powers several novel runtime monitoring and adaptation strategies. 
                  Apparate lowers median response latencies by 40.5-91.5% and 10.0-24.2% for diverse CV and NLP classification workloads, 
                  and median time-per-token latencies by 70.4-77.9% for generative scenarios, 
                  without affecting throughputs or violating tight accuracy constraints.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Improving DNN Inference Throughput Using Practical, Per-Input Compute Adaptation</h5> 
              Anand Iyer, Mingyu Guan, Yinwei Dai, Rui Pan, Swapnil Gandhi, Ravi Netravali <br>
              <a href="https://sigops.org/s/conferences/sosp/2024/" target="_blank">ACM SOSP 2024</a><br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <div class="mt-2">
                <a href="publications/sosp24_e3.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#e3-abstract" role="button" aria-expanded="false" aria-controls="e3-abstract">Abstract</a>
              </div>
              <div class="collapse" id="e3-abstract">
                <div class="card card-body">
                  Machine learning inference platforms continue to face high request rates and strict latency constraints. 
                  Existing solutions largely focus on compressing models to substantially lower compute costs (and time) with mild accuracy degradations. 
                  This paper explores an alternate (but complementary) technique that trades off accuracy and resource costs on a per-input granularity: 
                  early exit models, which selectively allow certain inputs to exit a model from an intermediate layer. 
                  Though intuitive, early exits face fundamental deployment challenges, largely owing to the effects that exiting inputs have on batch size (and resource utilization) 
                  throughout model execution. We present E3, the first system that makes early exit models practical for realistic inference deployments. 
                  Our key insight is to split and replicate blocks of layers in models in a manner that maintains a constant batch size throughout execution, 
                  all the while accounting for resource requirements and communication overheads. Evaluations with NLP and vision models show that E3 can deliver up to 1.74× 
                  improvement in goodput (for a fixed cost) or 1.78× reduction in cost (for a fixed goodput). 
                  Additionally, E3's goodput wins generalize to autoregressive LLMs (2.8-3.8×) and compressed models (1.67×).
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- 2023 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2023</h3>
          <ul>
            <li class="paper-item">
              <h5>Shockwave: Fair and Efficient Cluster Scheduling for Dynamic Adaptation in Machine Learning</h5> 
              Pengfei Zheng, Rui Pan, Tarannum Khan, Shivaram Venkataraman, Aditya Akella <br>
              <a href="https://www.usenix.org/conference/nsdi23" target="_blank">USENIX NSDI 2023</a><br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="publications/nsdi23_shockwave.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#shockwave-abstract" role="button" aria-expanded="false" aria-controls="shockwave-abstract">Abstract</a>
                <a href="https://github.com/uw-mad-dash/shockwave" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="shockwave-abstract">
                <div class="card card-body">
                  Dynamic adaptation has become an essential technique in accelerating distributed machine learning (ML) training: 
                    Recent studies have shown that dynamically adjusting model structure (e.g., lottery ticket hypothesis) or hyperparameters (e.g., batch size) 
                    can significantly accelerate training without sacrificing accuracy. However, existing ML cluster schedulers are not designed to handle dynamic adaptation. 
                    We show that existing schemes fail to provide fairness and degrade system efficiency when the training throughput changes over time under dynamic adaptation. 
                    We design Shockwave, a scheduler with future planning that builds on two key ideas. 
                    First, Shockwave extends classic market theory from static settings to dynamic settings to co-optimize efficiency and fairness. 
                    Second, Shockwave utilizes stochastic dynamic programming to handle uncertain, dynamic throughput. 
                    We build a system for Shockwave and validate its performance with both trace-driven simulation and cluster experiments. 
                    Results show that for traces of ML jobs with dynamic adaptation, Shockwave improves makespan by 1.3× and fairness by 2× when compared with existing fair scheduling schemes.
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>


    <!-- End-of-the-page jumbotron -->
    <div class="jumbotron text-center" style="padding-top: 10px; padding-bottom: 10px; margin-bottom:0">
      <div class="container">
        <div class="col-sm-12">
          <a href="https://github.com/SysML-Princeton" class="smlinks" target="_blank"><i class="fab fa-github-square fa-2x"></i></a>
        </div>
      </div>
      <p>@ 2025 SAIL@Princeton. Powered by Bootstrap.</p>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  </body>
</html>