<!DOCTYPE html>

<html lang="en-us">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no, user-scalable=no">

    <!-- Font Awesome for social media icons -->
    <script src="https://kit.fontawesome.com/791291c78f.js" crossorigin="anonymous"></script>

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

    <!-- Site Information -->
    <title> SAIL@Princeton </title>

    <style type="text/css">
      .smlinks {
        color: black;
      }
      .smlinks:hover {
        color: rgb(7, 107, 255);
      }
      .paper-item {
        margin-bottom: 15px; /* Adjust this value to increase/decrease the space */
      }
      .badge.badge-secondary {
        cursor: pointer;
      }
    </style>

    <!-- Favicon -->
    <!-- TODO(ruipan): we could add a favicon of the website here -->
    <!-- https://realfavicongenerator.net/ -->
    <!-- <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="theme-color" content="#ffffff"> -->
    
    <!-- Functionality for searching papers -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script>
      $(document).ready(function () {
          // Function to get URL parameters
          function getQueryParam(name) {
              let urlParams = new URLSearchParams(window.location.search);
              return urlParams.get(name) || "";
          }
    
          // Function to perform search
          function filterPapers(query) {
              query = query.toLowerCase();
              $(".paper-item").each(function () {
                  let text = $(this).text().toLowerCase();
                  $(this).closest("li").toggle(text.includes(query));
              });
          }
    
          // Populate search bar and apply filter if "search" parameter exists
          let searchQuery = getQueryParam("search");
          if (searchQuery) {
              $("#search").val(searchQuery);
              filterPapers(searchQuery); // Directly apply the filter
          }
    
          // Attach event listener for manual searches
          $("#search").on("input", function () {
              filterPapers($(this).val());
          });

          // Add click event to badge elements
          $(".badge.badge-secondary").on("click", function () {
              let keyword = $(this).text().trim();
              $("#search").val(keyword).trigger("input"); // Update search bar and trigger filtering
          });

          // Clear search when "Clear" button is clicked
          $("#clear-search").on("click", function () {
              $("#search").val("").trigger("input"); // Clear input and reset filter
          });
      });
    </script>
  </head>

  <body>
    <!-- Nav Bar -->
    <!-- TODO(ruipan): figure out how to align the nav items to the right rather than the left -->
    <nav class="navbar navbar-expand-lg navbar-light sticky-top navbar-custom" style="background-color: #f58025">
        <a class="navbar-brand" href="index.html">
          <img src="./images/princeton_square.jpg" width="30" height="30" class="d-inline-block align-top">
          SAIL@Princeton
        </a>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>

      <div class="collapse navbar-collapse" id="navbarSupportedContent">
        <ul class="navbar-nav mr-auto">
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="index.html#projects">Projects</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="people.html">People</a>
          </li>
          <li class="nav-item" data-toggle="collapse" data-target=".navbar-collapse.show">
            <a class="nav-link" href="publications.html">Publications</a>
          </li>
        </ul>
      </div>
    </nav>

    <!-- Jumbotron -->
    <div class="jumbotron jumbotron-fluid text-center">
      <div class="container">
        <div class="row align-items-center">
          <div class="col-sm-12">
            <h2 class="jumbotron-heading">Publications of SAIL@Princeton</h2>
            <p class="lead">Our publications showcase cutting-edge research at the intersection of systems and machine learning, 
              advancing efficient, scalable, and secure AI/ML systems. From novel models and algorithms to optimized runtime systems for training and inference, 
              our work pushes the boundaries of next-generation AI infrastructure. Explore our latest contributions to AI/ML and systems research below.</p>
          </div>
        </div>
      </div>
    </div>

    <!-- Search bar -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <div class="d-flex mb-3">
              <input type="text" id="search" class="form-control" placeholder="Search by title, author, or keyword..." style="flex: 1;">
              <button id="clear-search" class="btn btn-outline-secondary ml-2">Reset</button>
          </div>
        </div>
      </div>
    </div>


    <!-- Preprints -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>Preprints</h3>
          <ul>
            <li class="paper-item">
              <h5>How to Train Long-Context Language Models (Effectively)</h5> 
              Tianyu Gao*, Alexander Wettig*, Howard Yen, Danqi Chen <br>
              arXiv 2025<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://arxiv.org/abs/2410.02660" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#prolong-abstract" role="button" aria-expanded="false" aria-controls="prolong-abstract">Abstract</a>
              </div>
              <div class="collapse" id="prolong-abstract">
                <div class="card card-body">
                  We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. 
                  We first establish a reliable evaluation protocol to guide model development -- Instead of perplexity or simple needle-in-a-haystack (NIAH) tests, 
                  we use a broad set of long-context tasks, and we evaluate models after SFT with instruction data as this better reveals long-context abilities. 
                  Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, 
                  and many other design choices. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short data; 
                  (2) training with a sequence length beyond the evaluation length boosts long-context performance; 
                  (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. 
                  Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. 
                  ProLong outperforms Llama-3.18B-Instruct on the majority of long-context tasks despite having seen only 5% as many tokens during long-context training. 
                  Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Certifiably Robust RAG against Retrieval Corruption</h5> 
              Chong Xiang*, Tong Wu*, Zexuan Zhong, David Wagner, Danqi Chen, Prateek Mittal. <br>
              arXiv 2025<br>
              <span class="badge badge-secondary">Compound AI Systems</span>
              <div class="mt-2">
                <a href="https://arxiv.org/abs/2405.15556" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#robustrag-abstract" role="button" aria-expanded="false" aria-controls="robustrag-abstract">Abstract</a>
              </div>
              <div class="collapse" id="robustrag-abstract">
                <div class="card card-body">
                  Retrieval-augmented generation (RAG) has been shown vulnerable to retrieval corruption attacks: an attacker can inject malicious passages into retrieval results to induce inaccurate responses. 
                  In this paper, we propose RobustRAG as the first defense framework against retrieval corruption attacks. 
                  The key insight of RobustRAG is an isolate-then-aggregate strategy: we get LLM responses from each passage in isolation and then securely aggregate these isolated responses. 
                  To instantiate RobustRAG, we design keyword-based and decoding-based algorithms for securely aggregating unstructured text responses. 
                  Notably, RobustRAG can achieve certifiable robustness: we can formally prove and certify that, for certain queries, RobustRAG can always return accurate responses, 
                  even when the attacker has full knowledge of our defense and can arbitrarily inject a small number of malicious passages. We evaluate RobustRAG on open-domain QA and long-form text generation datasets and demonstrate its effectiveness and generalizability across various tasks and datasets.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation</h5> 
              Siddhant Ray, Rui Pan, Zhuohan Gu, Kuntai Du, Ganesh Ananthanarayanan, Ravi Netravali, Junchen Jiang <br>
              arXiv 2024<br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <span class="badge badge-secondary">Compound AI Systems</span>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2412.10543" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#ragserve-abstract" role="button" aria-expanded="false" aria-controls="ragserve-abstract">Abstract</a>
              </div>
              <div class="collapse" id="ragserve-abstract">
                <div class="card card-body">
                  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to 
                  generate better responses with external knowledge, but using more external 
                  knowledge often improves generation quality at the expense of response delay. 
                  Prior work either reduces the response delay (through better scheduling of RAG 
                  queries) or strives to maximize quality (which involves tuning the RAG workflow), 
                  but they fall short in optimizing the \emph {tradeoff} between the delay 
                  and quality of RAG responses. This paper presents RAGServe, the first RAG system 
                  that jointly schedules queries and adapts the key RAG configurations of each 
                  job, such as the number of retrieved text chunks and synthesis methods, 
                  in order to balance quality optimization and response delay reduction. 
                  Using 4 popular RAG-QA datasets, we show that compared with the state-of-the-art 
                  RAG scheduling system, RAGServe reduces the generation latency by 1.64--2.54×
                  without sacrificing generation quality.
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- 2025 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2025</h3>
          <ul>
            <li class="paper-item">
              <h5>Marconi: Prefix Caching for the Era of Hybrid LLMs</h5> 
              Rui Pan, Zhuang Wang, Zhen Jia, Can Karakus, Luca Zancato, Tri Dao, Yida Wang, Ravi Netravali <br>
              MLSys 2025<br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <span class="badge badge-secondary">State Space Models</span>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2411.19379" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#marconi-abstract" role="button" aria-expanded="false" aria-controls="marconi-abstract">Abstract</a>
                <a href="javascript:void(0);" target="_blank">
                  <button type="button" class="btn btn-outline-secondary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="marconi-abstract">
                <div class="card card-body">
                  Hybrid models that combine the language modeling capabilities of Attention layers with the efficiency of Recurrent
                  layers (e.g., State Space Models) have gained traction in practically supporting long contexts in Large Language
                  Model serving. Yet, the unique properties of these models complicate the usage of complementary efficiency
                  optimizations such as prefix caching that skip redundant computations across requests. Most notably, their use of
                  in-place state updates for recurrent layers precludes rolling back cache entries for partial sequence overlaps, and
                  instead mandates only exact-match cache hits; the effect is a deluge of (large) cache entries per sequence, most
                  of which yield minimal reuse opportunities. We present Marconi, the first system that supports efficient prefix
                  caching with Hybrid LLMs. Key to Marconi are its novel admission and eviction policies that more judiciously
                  assess potential cache entries based not only on recency, but also on (1) forecasts of their reuse likelihood across a
                  taxonomy of different hit scenarios, and (2) the compute savings that hits deliver relative to memory footprints.
                  Across diverse workloads and Hybrid models, Marconi achieves up to 34.4× higher token hit rates (71.1% or 617
                  ms lower TTFT) compared to state-of-the-art prefix caching systems.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Mowgli: A Passive Approach to Learning Rate Control for Real-Time Video</h5> 
              Neil Agarwal, Rui Pan, Francis Y. Yan, Ravi Netravali <br>
              NSDI 2025<br>
              <span class="badge badge-secondary">ML for Systems</span>
              <span class="badge badge-secondary">Edge AI Systems</span>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2410.03339" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#mowgli-abstract" role="button" aria-expanded="false" aria-controls="mowgli-abstract">Abstract</a>
              </div>
              <div class="collapse" id="mowgli-abstract">
                <div class="card card-body">
                  Rate control algorithms are at the heart of video conferencing platforms, 
                  determining target bitrates that match dynamic network characteristics for high quality. 
                  Recent data-driven strategies have shown promise for this challenging task, 
                  but the performance degradation they introduce during training has been a nonstarter 
                  for many production services, precluding adoption. 
                  This paper aims to bolster the practicality of data-driven rate control by presenting 
                  an alternative avenue for experiential learning: 
                  leveraging purely existing telemetry logs produced by the incumbent algorithm in production. 
                  We observe that these logs often contain effective decisions, although often at the wrong times or in the wrong order. 
                  To realize this approach despite the inherent uncertainty that log-based learning brings 
                  (i.e., lack of feedback for new decisions), our system, Mowgli, 
                  combines a variety of robust learning techniques (i.e., conservatively reasoning 
                  about alternate behavior to minimize risk and using a richer model formulation to account for environmental noise). 
                  Across diverse networks (emulated and real-world), Mowgli outperforms the widely deployed GCC algorithm, 
                  increasing average video bitrates by 15-39% while reducing freeze rates by 60-100%.
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- 2024 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2024</h3>
          <ul>
            <li class="paper-item">
              <h5>
                Apparate: Rethinking Early Exits to Tame Latency-Throughput Tensions in ML Serving
                <img src="images/acm_available_1.1.png" height="25"/><img src="images/acm_functional_1.1.png" height="25"/><img src="images/acm_reproduced_1.1.png" height="25"/>
              </h5> 
              Yinwei Dai*, Rui Pan*, Anand Iyer, Kai Li, Ravi Netravali <br>
              SOSP 2024<br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3694715.3695963" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#apparate-abstract" role="button" aria-expanded="false" aria-controls="apparate-abstract">Abstract</a>
                <a href="https://github.com/dywsjtu/apparate" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="apparate-abstract">
                <div class="card card-body">
                  Machine learning (ML) inference platforms are tasked with balancing two competing goals: 
                  ensuring high throughput given many requests, and delivering low-latency responses to support interactive applications. 
                  Unfortunately, existing platform knobs (e.g., batch sizes) fail to ease this fundamental tension, 
                  and instead only enable users to harshly trade off one property for the other. 
                  This paper explores an alternate strategy to taming throughput-latency tradeoffs by changing the granularity 
                  at which inference is performed. 
                  We present Apparate, a system that automatically applies and manages early exits (EEs) in ML models, 
                  whereby certain inputs can exit with results at intermediate layers. 
                  To cope with the time-varying overhead and accuracy challenges that EEs bring, 
                  Apparate repurposes exits to provide continual feedback that powers several novel runtime monitoring and adaptation strategies. 
                  Apparate lowers median response latencies by 40.5-91.5% and 10.0-24.2% for diverse CV and NLP classification workloads, 
                  and median time-per-token latencies by 70.4-77.9% for generative scenarios, 
                  without affecting throughputs or violating tight accuracy constraints.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Improving DNN Inference Throughput Using Practical, Per-Input Compute Adaptation</h5> 
              Anand Iyer, Mingyu Guan, Yinwei Dai, Rui Pan, Swapnil Gandhi, Ravi Netravali <br>
              SOSP 2024<br>
              <span class="badge badge-secondary">Efficient Inference</span>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3694715.3695978" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#e3-abstract" role="button" aria-expanded="false" aria-controls="e3-abstract">Abstract</a>
              </div>
              <div class="collapse" id="e3-abstract">
                <div class="card card-body">
                  Machine learning inference platforms continue to face high request rates and strict latency constraints. 
                  Existing solutions largely focus on compressing models to substantially lower compute costs (and time) with mild accuracy degradations. 
                  This paper explores an alternate (but complementary) technique that trades off accuracy and resource costs on a per-input granularity: 
                  early exit models, which selectively allow certain inputs to exit a model from an intermediate layer. 
                  Though intuitive, early exits face fundamental deployment challenges, largely owing to the effects that exiting inputs have on batch size (and resource utilization) 
                  throughout model execution. We present E3, the first system that makes early exit models practical for realistic inference deployments. 
                  Our key insight is to split and replicate blocks of layers in models in a manner that maintains a constant batch size throughout execution, 
                  all the while accounting for resource requirements and communication overheads. Evaluations with NLP and vision models show that E3 can deliver up to 1.74× 
                  improvement in goodput (for a fixed cost) or 1.78× reduction in cost (for a fixed goodput). 
                  Additionally, E3's goodput wins generalize to autoregressive LLMs (2.8-3.8×) and compressed models (1.67×).
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>SimPO: Simple Preference Optimization with a Reference-Free Reward</h5> 
              Yu Meng*, Mengzhou Xia*, Danqi Chen <br>
              NeurIPS 2024<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2405.14734" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#simpo-abstract" role="button" aria-expanded="false" aria-controls="simpo-abstract">Abstract</a>
              </div>
              <div class="collapse" id="simpo-abstract">
                <div class="card card-body">
                  Direct Preference Optimization (DPO) is a widely used offline preference optimization algorithm that reparameterizes reward functions in reinforcement learning from human feedback (RLHF) to enhance simplicity and training stability. 
                  In this work, we propose SimPO, a simpler yet more effective approach. The effectiveness of SimPO is attributed to a key design: using the average log probability of a sequence as the implicit reward. 
                  This reward formulation better aligns with model generation and eliminates the need for a reference model, making it more compute and memory efficient. 
                  Additionally, we introduce a target reward margin to the Bradley-Terry objective to encourage a larger margin between the winning and losing responses, further enhancing the algorithm's performance. 
                  We compare SimPO to DPO and its latest variants across various state-of-the-art training setups, including both base and instruction-tuned models like Mistral and Llama3. 
                  We evaluated on extensive instruction-following benchmarks, including AlpacaEval 2, MT-Bench, and the recent challenging Arena-Hard benchmark. 
                  Our results demonstrate that SimPO consistently and significantly outperforms existing approaches without substantially increasing response length. Specifically, SimPO outperforms DPO by up to 6.4 points on AlpacaEval 2 and by up to 7.5 points on Arena-Hard. 
                  Our top-performing model, built on Llama3-8B-Instruct, achieves a remarkable 44.7 length-controlled win rate on AlpacaEval 2 -- surpassing Claude 3 Opus on the leaderboard, and a 33.8 win rate on Arena-Hard -- making it the strongest 8B open-source model.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Lory: Fully Differentiable Mixture-of-Experts for Autoregressive Language Model Pre-training</h5> 
              Zexuan Zhong, Mengzhou Xia, Danqi Chen, Mike Lewis <br>
              COLM 2024<br>
              <span class="badge badge-secondary">Emerging Paradigms</span>
              <div class="mt-2">
                <a href="https://arxiv.org/abs/2405.03133" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#lory-abstract" role="button" aria-expanded="false" aria-controls="lory-abstract">Abstract</a>
              </div>
              <div class="collapse" id="lory-abstract">
                <div class="card card-body">
                  Mixture-of-experts (MoE) models facilitate efficient scaling; however, training the router network introduces the challenge of optimizing a non-differentiable, discrete objective. 
                  Recently, a fully-differentiable MoE architecture, SMEAR, was proposed (Muqeeth et al., 2023), which softly merges experts in the parameter space; nevertheless, its effectiveness was only demonstrated in downstream fine-tuning on classification tasks. 
                  In this paper, we present Lory, the first approach that scales such architectures to autoregressive language model pre-training. 
                  Lory introduces two key techniques: (1) a causal segment routing strategy that achieves high efficiency for expert merging operations while preserving the autoregressive nature of language models; 
                  (2) a similarity-based data batching method that encourages expert specialization by grouping similar documents in training instances. 
                  We pre-train a series of Lory models on 150B tokens from scratch, with up to 32 experts and 30B (1.5B active) parameters. 
                  Experimental results show significant performance gains over parameter-matched dense models on both perplexity (+13.9%) and a variety of downstream tasks (+1.5%-11.1%). 
                  Despite segment-level routing, Lory models achieve competitive performance compared to state-of-the-art MoE models with token-level routing. We further demonstrate that the trained experts in Lory capture domain-level specialization without supervision. 
                  Our work highlights the potential of fully-differentiable MoE architectures for language model pre-training and advocates future research in this area.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning</h5> 
              Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, Danqi Chen<br>
              ICLR 2024<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://arxiv.org/abs/2310.06694" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#sheared-llama-abstract" role="button" aria-expanded="false" aria-controls="sheared-llama-abstract">Abstract</a>
              </div>
              <div class="collapse" id="sheared-llama-abstract">
                <div class="card card-body">
                  The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged moderate-sized large language models (LLMs) highlights the potential of building smaller yet powerful LLMs. 
                  Regardless, the cost of training such models from scratch on trillions of tokens remains high. In this work, we study structured pruning as an effective means to develop smaller LLMs from pre-trained, larger models. 
                  Our approach employs two key techniques: (1) targeted structured pruning, which prunes a larger model to a specified target shape by removing layers, heads, and intermediate and hidden dimensions in an end-to-end manner, 
                  and (2) dynamic batch loading, which dynamically updates the composition of sampled data in each training batch based on varying losses across different domains. We demonstrate the efficacy of our approach by presenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B and 2.7B parameters. 
                  Sheared-LLaMA models outperform state-of-the-art open-source models of equivalent sizes, such as Pythia, INCITE, and OpenLLaMA models, on a wide range of downstream and instruction tuning evaluations, while requiring only 3% of compute compared to training such models from scratch. 
                  This work provides compelling evidence that leveraging existing LLMs with structured pruning is a far more cost-effective approach for building smaller LLMs.
                </div>
              </div>
            </li>
          </ul>
        </div>
      </div>
    </div>

    <!-- 2023 -->
    <div class="container">
      <div class="row">
        <div class="col-sm-12">
          <h3>2023</h3>
          <ul>
            <li class="paper-item">
              <h5>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</h5> 
              Albert Gu*, Tri Dao* <br>
              COLM 2023<br>
              <span class="badge badge-secondary">State Space Models</span>
              <span class="badge badge-secondary">Emerging Paradigms</span>
              <div class="mt-2">
                <a href="https://arxiv.org/pdf/2312.00752" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#mamba-abstract" role="button" aria-expanded="false" aria-controls="mamba-abstract">Abstract</a>
                <a href="https://github.com/state-spaces/mamba/" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="mamba-abstract">
                <div class="card card-body">
                  Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5× higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Shockwave: Fair and Efficient Cluster Scheduling for Dynamic Adaptation in Machine Learning</h5> 
              Pengfei Zheng, Rui Pan, Tarannum Khan, Shivaram Venkataraman, Aditya Akella <br>
              NSDI 2023<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://www.usenix.org/system/files/nsdi23-zheng.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#shockwave-abstract" role="button" aria-expanded="false" aria-controls="shockwave-abstract">Abstract</a>
                <a href="https://github.com/uw-mad-dash/shockwave" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="shockwave-abstract">
                <div class="card card-body">
                  Dynamic adaptation has become an essential technique in accelerating distributed machine learning (ML) training: 
                  Recent studies have shown that dynamically adjusting model structure (e.g., lottery ticket hypothesis) or hyperparameters (e.g., batch size) 
                  can significantly accelerate training without sacrificing accuracy. However, existing ML cluster schedulers are not designed to handle dynamic adaptation. 
                  We show that existing schemes fail to provide fairness and degrade system efficiency when the training throughput changes over time under dynamic adaptation. 
                  We design Shockwave, a scheduler with future planning that builds on two key ideas. 
                  First, Shockwave extends classic market theory from static settings to dynamic settings to co-optimize efficiency and fairness. 
                  Second, Shockwave utilizes stochastic dynamic programming to handle uncertain, dynamic throughput. 
                  We build a system for Shockwave and validate its performance with both trace-driven simulation and cluster experiments. 
                  Results show that for traces of ML jobs with dynamic adaptation, Shockwave improves makespan by 1.3× and fairness by 2× when compared with existing fair scheduling schemes.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>ModelKeeper: Accelerating DNN Training via Automated Training Warmup</h5> 
              Fan Lai, Yinwei Dai, Harsha Madhyastha, Mosharaf Chowdhury <br>
              NSDI 2023<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://www.usenix.org/system/files/nsdi23-lai-fan.pdf" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#modelkeeper-abstract" role="button" aria-expanded="false" aria-controls="modelkeeper-abstract">Abstract</a>
                <a href="https://github.com/SymbioticLab/ModelKeeper" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">Code</button>
                </a>
              </div>
              <div class="collapse" id="modelkeeper-abstract">
                <div class="card card-body">
                  With growing deployment of machine learning (ML) models,
                  ML developers are training or re-training increasingly more
                  deep neural networks (DNNs). They do so to find the most
                  suitable model that meets their accuracy requirement while
                  satisfying the resource and timeliness constraints of the target
                  environment. In large shared clusters, the growing number
                  of neural architecture search (NAS) and training jobs often
                  result in models sharing architectural similarities with others
                  from the same or a different ML developer. However, existing
                  solutions do not provide a systematic mechanism to identify
                  and leverage such similarities.
                  We present ModelKeeper, the first automated training
                  warmup system that accelerates DNN training by repurposing previously-trained models in a shared cluster. Our key
                  insight is that initializing a training job’s model by transforming an already-trained model’s weights can jump-start it and
                  reduce the total amount of training needed. However, models submitted over time can differ in their architectures and
                  accuracy. Given a new model to train, ModelKeeper scalably
                  identifies its architectural similarity with previously trained
                  models, selects a parent model with high similarity and good
                  model accuracy, and performs structure-aware transformation
                  of weights to preserve maximal information from the parent
                  model during the warmup of new model weights. Our evaluations across thousands of CV and NLP models show that
                  ModelKeeper achieves 1.3×–4.3× faster training completion
                  with little overhead and no reduction in model accuracy.
                </div>
              </div>
            </li>
            <li class="paper-item">
              <h5>Auxo: Efficient Federated Learning via Scalable Client Clustering</h5> 
              Jiachen Liu, Fan Lai, Yinwei Dai, Aditya Akella, Harsha Madhyastha, Mosharaf Chowdhury <br>
              SoCC 2023<br>
              <span class="badge badge-secondary">Efficient Training</span>
              <div class="mt-2">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3620678.3624651" target="_blank">
                  <button type="button" class="btn btn-outline-primary btn-sm">PDF</button>
                </a>
                <a class="btn btn-outline-primary btn-sm" data-toggle="collapse" href="#auxo-abstract" role="button" aria-expanded="false" aria-controls="auxo-abstract-abstract">Abstract</a>
              </div>
              <div class="collapse" id="auxo-abstract">
                <div class="card card-body">
                  Federated learning (FL) is an emerging machine learning
                  (ML) paradigm that enables heterogeneous edge devices to
                  collaboratively train ML models without revealing their raw
                  data to a logically centralized server. However, beyond the
                  heterogeneous device capacity, FL participants often exhibit
                  differences in their data distributions, which are not independent and identically distributed (Non-IID). Many existing
                  works present point solutions to address issues like slow
                  convergence, low final accuracy, and bias in FL, all stemming
                  from client heterogeneity.
                  In this paper, we explore an additional layer of complexity to mitigate such heterogeneity by grouping clients with
                  statistically similar data distributions (cohorts). We propose
                  Auxo to gradually identify such cohorts in large-scale, lowavailability, and resource-constrained FL populations. Auxo
                  then adaptively determines how to train cohort-specific models in order to achieve better model performance and ensure
                  resource efficiency. Our extensive evaluations show that,
                  by identifying cohorts with smaller heterogeneity and performing efficient cohort-based training, Auxo boosts various
                  existing FL solutions in terms of final accuracy (2.1%–8.2%),
                  convergence time (up to 2.2×), and model bias (4.8% - 53.8%)
                </div>
              </div>
            </li>


          </ul>
        </div>
      </div>
    </div>


    <!-- End-of-the-page jumbotron -->
    <div class="jumbotron text-center" style="padding-top: 10px; padding-bottom: 10px; margin-bottom:0">
      <div class="container">
        <div class="col-sm-12">
          <a href="https://github.com/SysML-Princeton" class="smlinks" target="_blank"><i class="fab fa-github-square fa-2x"></i></a>
        </div>
      </div>
      <p>@ 2025 SAIL@Princeton. Powered by Bootstrap.</p>
    </div>

    <!-- Optional JavaScript -->
    <!-- jQuery first, then Popper.js, then Bootstrap JS -->
    <script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js" integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  </body>
</html>